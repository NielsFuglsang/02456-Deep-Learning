{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for the final report in 02456 Deep Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Authors:** *Mads Emil Dahlgaard (S164206), Morten Wehlast Jørgensen (S147056), and Niels Asp Fuglsang (S164181)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook explains the code and recreates the main results from the report *An analysis of levers in deep reinforcement learning and how they affect learning speed and generalization*. The full code base can be found at https://github.com/NielsFuglsang/02456-Deep-Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep reinforcement learning is notourisly known for results that are difficult to reproduce. Even very small changes to hyperparameters can lead to completely different results. Therefore, in this project we set out to investigate how to best increase one's chances of improving model performance. We identified the following 3 areas.\n",
    "- Data set size\n",
    "- Input transformation\n",
    "- Choice of policy optimization algorithm\n",
    "We believe that these three areas are important for any reinforcement learning practioner and particularly for newcomers to the field.\n",
    "\n",
    "In order to investigate this we use the Starpilot environment from ProcGen Benchmark. The procedurally generated environment allows for virtually unlimited training data and is a perfect starting point for investigating the importance of data set size. In order to explore input transformation we chose to focus on to different convolutional neural networks; the simple Nature CNN and the more advanced IMPALA CNN. Lastly, we compare the state-of-the-art policy optimization algorithms PPO and TRPO. To sum up, we conclude the following three points.\n",
    "- Data set size: larger volumes of varied data generally improves generalization\n",
    "- Encoders: IMPALA CNN outperforms Nature CNN even on smaller tasks\n",
    "- Policy optimization: PPO is both simpler and better performing than TRPO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technical stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Python version is 3.7.7 64 bit. All experiments are run on DTU HPC GPU nodes. Specifically, the experiments are run on the `gpuv100` queue consisting of 10 nodes with 2 Nvidia Tesla V100 Tensor Core GPUs - 6 of the nodes have 16GB ram and 4 of the nodes 32GB. The following list contains the most significant modules used. For a full list see `requirements.txt` in the Github repository.\n",
    "- CUDA 9.2\n",
    "- CuDNN 7.4.2.24\n",
    "- Jupyter Lab 2.2.9\n",
    "- Matplotlib 3.3.3\n",
    "- Numpy 1.19.4\n",
    "- OpenAI Gym 0.17.3\n",
    "- Procgen 0.10.4\n",
    "- PyTorch 1.7.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tried to make our code as modular as possible. The overall code structure looks like this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "├── src\n",
    "│   └── init.py\n",
    "│   └── encoder.py         # Classes for each encoder structure.\n",
    "│   └── experiment.py      # Class for training and evaluating a policy.\n",
    "│   └── policy.py          # PPO and TRPO classes. \n",
    "│   └── utils.py           # Functions for keeping track of environments and data.\n",
    "├── params\n",
    "│   └── ...                # JSON files specifying experiment hyperparameters.\n",
    "├── .gitignore\n",
    "├── README.md\n",
    "├── job.sh                 # Jobscript for running on HPC.\n",
    "├── requirements.txt       # Python modules.\n",
    "├── run_experiment.py      # Read parameters from JSON file, train, and evaluate policy.\n",
    "├── sender.sh              # Helper function to submit jobs on HPC.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The folder `src` contains the main code for this project and is structured as a Python package. The file `encoder.py` implements the two Convolutional Neural Networks used in this project; Impala CNN and Nature CNN. The file `policy.py` contains classes for `PPO` and `TRPO` respectively with two main class methods; `act` for using the policy and sampling an action, and `loss` for returning the policy loss given some observation.\n",
    "\n",
    "The file `experiment.py` contains code for training and evaluating a policy in an environment. The `Experiment` class from `experiment.py` is independent from the choice of encoder and policy. This means that the same code can be used to train both a TRPO and a PPO policy. Therefore, we just need to specify the encoder, the policy, and the hyperparameters when creating the `Experiment` class. As an example, in order to train a PPO network with the IMPALA CNN as encoder the following code is sufficient.\n",
    "\n",
    "```python\n",
    "exp = Experiment(params)\n",
    "encoder = Impala(in_channels, feature_dim)\n",
    "policy = PPO(encoder, feature_dim, num_actions)\n",
    "policy, log = exp.train(env, policy, optimizer, storage)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the variables `params`, `in_channels`, `feature_dim`, and so on. They are all hyperparameters needed to train and evaluate the policy. The combination of policy optimization algorithm, encoder, and hyperparameters is essentially what defines an experiment. We came up with a way to specify all the parameters needed to run an experiment, which enabled us to easily conduct multiple different experiments without changing the code. All the experiments are specified as JSON files in the folder `params`. An experiment could look like this.\n",
    "\n",
    "```JSON\n",
    "{\n",
    "    \"total_steps\" : 2e6,\n",
    "    \"num_envs\": 32,\n",
    "    \"num_levels\": 10,\n",
    "    \"num_steps\": 256,\n",
    "    \"num_epochs\": 3,\n",
    "    \"batch_size\": 512,\n",
    "    \"eps\": 0.2,\n",
    "    \"grad_eps\": 0.5,\n",
    "    \"value_coef\": 0.5,\n",
    "    \"entropy_coef\": 0.01,\n",
    "    \"feature_dim\": 128,\n",
    "    \"policy\": \"ppo\",\n",
    "    \"encoder\": \"nature\",\n",
    "    \"beta\": 0,\n",
    "    \"lr\": 5e-4\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Python file `run_experiment.py` then takes as argument the filename of a JSON file with experiment parameters and runs the entire pipeline, i.e., train, evaluate, and save the results. If the above parameters are stored in `params/experiment1.json`, then the experiment can be executed by the following command.\n",
    "```sh\n",
    ">> python run_experiment.py experiment1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the code on DTU HPC\n",
    "\n",
    "We needed to specify the above as a jobscript in order to run the experiments on the DTU HPC cluster queue system. The jobscript is shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sh\n",
    "#!/bin/sh\n",
    "#BSUB -q gpuv100\n",
    "#BSUB -gpu \"num=1\"\n",
    "#BSUB -J name\n",
    "#BSUB -n 1\n",
    "#BSUB -W 10:00\n",
    "#BSUB -R \"rusage[mem=32GB]\"\n",
    "#BSUB -o logs/name.out\n",
    "#BSUB -e logs/name.err\n",
    "\n",
    "cd /zhome/ff/2/118359/projects/02456-Deep-Learning\n",
    "source .venv/bin/activate\n",
    "\n",
    "echo \"Running script...\"\n",
    "python run_experiment.py name\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now in order to simplify this even further we created a small shell script that exchanges `name` in the above with an input argument and submits it to the queue using `bsub`. This script is called `sender.sh`. Therefore, one can simply conduct an experiment by creating a parameters JSON file and submitting the experiment like this\n",
    "```sh\n",
    "source sender.sh name_of_experiment\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This way we could easily specify and run the different experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the training loop we both generate the training data and train on that data. This is possible since we are dealing with a procedurally generated environment. Therefore, the training loop goes like this.\n",
    "- Generate `num_steps` of training data.\n",
    "- Go over that training data for `num_epochs`.\n",
    "     - For each epoch calculate the loss, perform backward propagation, and update the policy.\n",
    "- Generate new data and evaluate the test reward.\n",
    "\n",
    "The above loop is run as long as the acummulated number of steps is less than `num_steps`. Notice that this is different from most real life machine learning since we are able to simply generate new data. Furthermore, the above is run simultaneously in parallel for 32 environments to speed up the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to compare training and test performance it is important that these two metrics are evaluated in the same way. We therefore use mean episodic reward. The mean episodic reward is calculated by running all 32 environments for one episode and taking the average reward over the 32 environemnts. One episode is defined from when the agent starts playing until it fails. Here is a code snippet showing how this is implemented.\n",
    "```python\n",
    "workers_finished = np.zeros((self.num_envs), dtype=bool)\n",
    "while not np.all(workers_finished):\n",
    "\n",
    "    # Use policy.\n",
    "    action, _, _, _ = policy.act(obs)\n",
    "\n",
    "    # Take step in environment.\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    for i in range(self.num_envs):\n",
    "        if done[i]:\n",
    "            workers_finished[i] = True\n",
    "        if workers_finished[i]:\n",
    "            reward[i] = 0\n",
    "\n",
    "    total_reward.append(torch.Tensor(reward))\n",
    "   \n",
    "# Calculate average reward\n",
    "mean_reward = torch.stack(total_reward).sum(0).mean(0)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This also means that the same code can be used to evaluate both training and test mean episodic reward simply by specifying what levels the agent should be evaluated on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the results and logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each experiment we save a log of the progress. This log looks like this\n",
    "```python\n",
    "log = {\n",
    "    'step': steps,\n",
    "    'train_mean_reward': train_mean_reward,\n",
    "    'train_min_reward': train_min_reward,\n",
    "    'train_max_reward': train_max_reward,\n",
    "    'test_mean_reward': test_mean_reward,\n",
    "    'test_min_reward': test_min_reward,\n",
    "    'test_max_reward': test_max_reward,\n",
    "    'pi_loss': pi_loss,\n",
    "    'value_loss': value_loss,\n",
    "    'entropy_loss': entropy_loss,\n",
    "    'test_var': test_vars,\n",
    "    'train_var': train_vars\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where each variable is a list of size `total_steps/num_steps`, i.e., the number of training loops. We use the function `torch.save` to save the logging files in the pickle serialization format. The results are saved to the `results` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recreating the main results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is unatainable to show all of the main results here since the experiments were run on the DTU HPC cluster with a compute time of multiple hours. Therefore, we will here show a much smaller experiment that showcases the procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import the relevant modules. We need `torch` and then some custom modules contained in the `src` folder. We important the `Experiment` class and the classes for the encoders and the policies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from src.utils import make_env, Storage\n",
    "from src import Experiment, Nature, Impala, PPO, TRPO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to specify the experiment. Here a relatively simple experiment is specified. We only run the experiment for 1 million timesteps and with 10 training levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"total_steps\" : 1e6,\n",
    "    \"num_envs\": 32,\n",
    "    \"num_levels\": 10,\n",
    "    \"num_steps\": 256,\n",
    "    \"num_epochs\": 3,\n",
    "    \"batch_size\": 512,\n",
    "    \"eps\": 0.2,\n",
    "    \"grad_eps\": 0.5,\n",
    "    \"value_coef\": 0.5,\n",
    "    \"entropy_coef\": 0.01,\n",
    "    \"feature_dim\": 32,\n",
    "    \"policy\": \"ppo\",\n",
    "    \"encoder\": \"impala\",\n",
    "    \"beta\": 0,\n",
    "    \"lr\": 5e-4\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above dictionary is the only thing that needs to be changed in order to conduct a different experiment - this makes it easy to try out different hypotheses. \n",
    "\n",
    "Next we create the Procgen Starpilot training environment. We specify that we want to train on the levels from 0 to `num_levels` which in this case is 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_env(params[\"num_envs\"], env_name='starpilot', start_level=0, num_levels=params[\"num_levels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then define the encoder. This is done in the following way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dim = params['feature_dim']\n",
    "in_channels = 3 # RGB\n",
    "encoders = {\n",
    "    \"nature\": Nature(in_channels, feature_dim),\n",
    "    \"impala\": Impala(in_channels, feature_dim)\n",
    "    }\n",
    "encoder = encoders[params['encoder']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The policy is defined in a similar way. Furthermore, the policy is moved to the GPU by the command `policy.cuda()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_actions = env.action_space.n\n",
    "policies = {\n",
    "    \"ppo\": PPO(encoder, feature_dim, num_actions),\n",
    "    \"trpo\": TRPO(encoder, feature_dim, num_actions, beta=params['beta'])\n",
    "}\n",
    "policy = policies[params['policy']]\n",
    "policy.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout all experiments we have used the Adam optimizer. In our final experiments we have kept the learning rate consistent to avoid too many variying factors that could influence the results. Similarly, we have not utilized weight decay. The optimizer is defined as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(policy.parameters(), lr=params['lr'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then define a temporary storage for collecting and keeping track of transitions during each training iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage = Storage(\n",
    "    obs_shape=env.observation_space.shape,\n",
    "    num_steps=params[\"num_steps\"],\n",
    "    num_envs=params[\"num_envs\"],\n",
    "    act_shape=num_actions\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The experiment class is then initialized. This is were the remaining parameters in the `param` dictionary become important. Variables such as `total_steps`, `num_steps`, and `batch_size` are used in the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = Experiment(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can train the policy and collect the log. Notice the `verbose=True` argument enables intermediate printing to the console. This would normally be set to false except when debugging or explicitly showing results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-50469f92f222>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/dtu-courses/git-repos/02456-Deep-Learning/src/experiment.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, env, policy, optimizer, storage, verbose)\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                 \u001b[0;31m# Use policy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m                 \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0;31m# Take step in environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/dtu-courses/git-repos/02456-Deep-Learning/src/policy.py\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;34m\"\"\"Class act method.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/dtu-courses/git-repos/02456-Deep-Learning/.venv/lib/python3.7/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \"Cannot re-initialize CUDA in forked subprocess. \" + msg)\n\u001b[1;32m    165\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_cuda_getDeviceCount'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Torch not compiled with CUDA enabled\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_cudart\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             raise AssertionError(\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "policy, log = exp.train(env, policy, optimizer, storage, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then save the results. We both save the policy and the log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'log' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-7f142b2ef402>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Save logging.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'overview.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'log' is not defined"
     ]
    }
   ],
   "source": [
    "torch.save(policy.state_dict, 'overview-policy.pt') # Save policy.\n",
    "torch.save(log, 'overview.pt') # Save logging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The folder `results` contains python scripts for plotting the figures in our poster and report. Below we will plot a figure for the small experiment we just carried out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapping up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we have described the overall structure of the code and the experiments. All of the experiments carried out can be found in the folder `params` - we carried out more than 51 experiments with compute times ranging from 1.5 to 8 hours. The results and the scripts for visualizing can be found in the folder `results` in the pickle serialization format.\n",
    "\n",
    "The code can be found here: https://github.com/NielsFuglsang/02456-Deep-Learning.\n",
    "\n",
    "Videos of the agents playing can be seen here: bit.ly/39OGphQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
