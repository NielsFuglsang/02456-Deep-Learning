{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for the final report in 02456 Deep Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Authors:** *Mads Emil Dahlgaard (S164206), Morten Wehlast Jørgensen (S147056), and Niels Asp Fuglsang (S164181)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook recreates the main results from the report *An analysis of levers in deep reinforcement learning and how they affect learning speed and generalization*. The full code base can be found at https://github.com/NielsFuglsang/02456-Deep-Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep reinforcement learning is notourisly known for results that are difficult to reproduce. Even very small changes to hyperparameters can lead to completely different results. Therefore, in this project we set out to investigate how to best increase one's chances of improving model performance. We identified the following 3 areas.\n",
    "- Data set size\n",
    "- Input transformation\n",
    "- Choice of policy optimization algorithm\n",
    "We believe that these three areas are important for any reinforcement learning practioner and particularly for newcomers to the field.\n",
    "\n",
    "In order to investigate this we use the Starpilot environment from ProcGen Benchmark. The procedurally generated environment allows for virtually unlimited training data and is a perfect starting point for investigating the importance of data set size. In order to explore input transformation we chose to focus on to different convolutional neural networks; the simple Nature CNN and the more advanced IMPALA CNN. Lastly, we compare the state-of-the-art policy optimization algorithms PPO and TRPO. To sum up, we conclude the following three points.\n",
    "- Data set size: larger volumes of varied data generally improves generalization\n",
    "- Encoders: IMPALA CNN outperforms Nature CNN even on smaller tasks\n",
    "- Policy optimization: PPO is both simpler and better performing than TRPO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technical stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Python version is 3.7.7 64 bit. All experiments are run on DTU HPC GPU nodes. Specifically, the experiments are run on the `gpuv100` queue consisting of 10 nodes with 2 Nvidia Tesla V100 Tensor Core GPUs - 6 of the nodes have 16GB ram and 4 of the nodes 32GB. The following list contains the most significant modules used. For a full list see `requirements.txt` in the Github repository.\n",
    "- CUDA 9.2\n",
    "- CuDNN 7.4.2.24\n",
    "- Jupyter Lab 2.2.9\n",
    "- Matplotlib 3.3.3\n",
    "- Numpy 1.19.4\n",
    "- OpenAI Gym 0.17.3\n",
    "- Procgen 0.10.4\n",
    "- PyTorch 1.7.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tried to make our code as modular as possible. The overall code structure looks like this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "├── src\n",
    "│   └── init.py\n",
    "│   └── encoder.py         # Classes for each encoder structure.\n",
    "│   └── experiment.py      # Class for training and evaluating a policy.\n",
    "│   └── policy.py          # PPO and TRPO classes. \n",
    "│   └── utils.py           # Functions for keeping track of environments and data.\n",
    "├── params\n",
    "│   └── ...                # JSON files specifying experiment hyperparameters.\n",
    "├── .gitignore\n",
    "├── README.md\n",
    "├── job.sh                 # Jobscript for running on HPC.\n",
    "├── requirements.txt       # Python modules.\n",
    "├── run_experiment.py      # Read parameters from JSON file, train, and evaluate policy.\n",
    "├── sender.sh              # Helper function to submit jobs on HPC.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The folder `src` contains the main code for this project and is structured as a Python package. The file `encoder.py` implements the two Convolutional Neural Networks used in this project; Impala CNN and Nature CNN. The file `policy.py` contains classes for `PPO` and `TRPO` respectively with two main class methods; `act` for using the policy and sampling an action, and `loss` for returning the policy loss given some observation.\n",
    "\n",
    "The file `experiment.py` contains code for training and evaluating a policy in an environment. The `Experiment` class from `experiment.py` is independent from the choice of encoder and policy. This means that the same code can be used to train both a TRPO and a PPO policy. Therefore, we just need to specify the encoder, the policy, and the hyperparameters when creating the `Experiment` class. As an example, in order to train a PPO network with the IMPALA CNN as encoder the following code is sufficient.\n",
    "\n",
    "```python\n",
    "exp = Experiment(params)\n",
    "encoder = Impala(in_channels, feature_dim)\n",
    "policy = PPO(encoder, feature_dim, num_actions)\n",
    "policy, log = exp.train(env, policy, optimizer, storage)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the variables `params`, `in_channels`, `feature_dim`, and so on. They are all hyperparameters needed to train and evaluate the policy. The combination of policy optimization algorithm, encoder, and hyperparameters is essentially what defines an experiment. We came up with a way to specify all the parameters needed to run an experiment, which enabled us to easily conduct multiple different experiments without changing the code. All the experiments are specified as JSON files in the folder `params`. An experiment could look like this.\n",
    "\n",
    "```JSON\n",
    "{\n",
    "    \"total_steps\" : 2e6,\n",
    "    \"num_envs\": 32,\n",
    "    \"num_levels\": 10,\n",
    "    \"num_steps\": 256,\n",
    "    \"num_epochs\": 3,\n",
    "    \"batch_size\": 512,\n",
    "    \"eps\": 0.2,\n",
    "    \"grad_eps\": 0.5,\n",
    "    \"value_coef\": 0.5,\n",
    "    \"entropy_coef\": 0.01,\n",
    "    \"feature_dim\": 128,\n",
    "    \"policy\": \"ppo\",\n",
    "    \"encoder\": \"nature\",\n",
    "    \"beta\": 0,\n",
    "    \"lr\": 5e-4\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Python file `run_experiment.py` then takes as argument the filename of a JSON file with experiment parameters and runs the entire pipeline, i.e., train, evaluate, and save the results. If the above parameters are stored in `params/experiment1.json`, then the experiment can be executed by the following command.\n",
    "```sh\n",
    ">> python run_experiment.py experiment1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the code on DTU HPC\n",
    "\n",
    "We needed to specify the above as a jobscript in order to run the experiments on the DTU HPC cluster queue system. The jobscript is shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sh\n",
    "#!/bin/sh\n",
    "#BSUB -q gpuv100\n",
    "#BSUB -gpu \"num=1\"\n",
    "#BSUB -J name\n",
    "#BSUB -n 1\n",
    "#BSUB -W 10:00\n",
    "#BSUB -R \"rusage[mem=32GB]\"\n",
    "#BSUB -o logs/name.out\n",
    "#BSUB -e logs/name.err\n",
    "\n",
    "cd /zhome/ff/2/118359/projects/02456-Deep-Learning\n",
    "source .venv/bin/activate\n",
    "\n",
    "echo \"Running script...\"\n",
    "python run_experiment.py name\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now in order to simplify this even further we created a small shell script that exchanges `name` in the above with an input argument and submits it to the queue using `bsub`. This script is called `sender.sh`. Therefore, one can simply conduct an experiment by creating a parameters JSON file and submitting the experiment like this\n",
    "```sh\n",
    "source sender.sh name_of_experiment\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This way we could easily specify and run the different experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the training loop we both generate the training data and train on that data. This is possible since we are dealing with a procedurally generated environment. Therefore, the training loop goes like this.\n",
    "- Generate `num_steps` of training data.\n",
    "- Go over that training data for `num_epochs`.\n",
    "     - For each epoch calculate the loss, perform backward propagation, and update the policy.\n",
    "- Generate new data and evaluate the test reward.\n",
    "\n",
    "The above loop is run as long as the acummulated number of steps is less than `num_steps`. Notice that this is different from most real life machine learning since we are able to simply generate new data. Furthermore, the above is run simultaneously in parallel for 32 environments to speed up the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to compare training and test performance it is important that these two metrics are evaluated in the same way. We therefore use mean episodic reward. The mean episodic reward is calculated by running all 32 environments for one episode and taking the average reward over the 32 environemnts. One episode is defined from when the agent starts playing until it fails. Here is a code snippet showing how this is implemented.\n",
    "```python\n",
    "workers_finished = np.zeros((self.num_envs), dtype=bool)\n",
    "while not np.all(workers_finished):\n",
    "\n",
    "    # Use policy.\n",
    "    action, _, _, _ = policy.act(obs)\n",
    "\n",
    "    # Take step in environment.\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    for i in range(self.num_envs):\n",
    "        if done[i]:\n",
    "            workers_finished[i] = True\n",
    "        if workers_finished[i]:\n",
    "            reward[i] = 0\n",
    "\n",
    "    total_reward.append(torch.Tensor(reward))\n",
    "   \n",
    "# Calculate average reward\n",
    "mean_reward = torch.stack(total_reward).sum(0).mean(0)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This also means that the same code can be used to evaluate both training and test mean episodic reward simply by specifying what levels the agent should be evaluated on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the results and logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each experiment we save a log of the progress. This log looks like this\n",
    "```python\n",
    "log = {\n",
    "    'step': steps,\n",
    "    'train_mean_reward': train_mean_reward,\n",
    "    'train_min_reward': train_min_reward,\n",
    "    'train_max_reward': train_max_reward,\n",
    "    'test_mean_reward': test_mean_reward,\n",
    "    'test_min_reward': test_min_reward,\n",
    "    'test_max_reward': test_max_reward,\n",
    "    'pi_loss': pi_loss,\n",
    "    'value_loss': value_loss,\n",
    "    'entropy_loss': entropy_loss,\n",
    "    'test_var': test_vars,\n",
    "    'train_var': train_vars\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where each variable is a list of size `totalSteps/numSteps`, i.e., the number of training loops. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
